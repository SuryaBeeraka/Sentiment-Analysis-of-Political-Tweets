{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f172f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/suryabeeraka/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/suryabeeraka/anaconda3/lib/python3.11/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/suryabeeraka/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/suryabeeraka/anaconda3/lib/python3.11/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/suryabeeraka/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef75ebdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "783d9ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama Tweets:\n",
      "   Unnamed: 0                 date            time  \\\n",
      "0         NaN                  NaN             NaN   \n",
      "1         NaN  2012-10-16 00:00:00  10:28:53-05:00   \n",
      "2         NaN  2016-12-10 00:00:00  10:09:00-05:00   \n",
      "3         NaN  2012-10-16 00:00:00  10:04:30-05:00   \n",
      "4         NaN  2012-10-16 00:00:00  10:00:36-05:00   \n",
      "\n",
      "                                     Anootated tweet Unnamed: 4  Unnamed: 5  \n",
      "0    1: positive, -1: negative, 0: neutral, 2: mixed      Class  Your class  \n",
      "1  Kirkpatrick, who wore a baseball cap embroider...          0         NaN  \n",
      "2  Question: If <e>Romney</e> and <e>Obama</e> ha...          2         NaN  \n",
      "3  #<e>obama</e> debates that Cracker Ass Cracker...          1         NaN  \n",
      "4  RT @davewiner Slate: Blame <e>Obama</e> for fo...          2         NaN   \n",
      "\n",
      "Romney Tweets:\n",
      "   Unnamed: 0                 date            time  \\\n",
      "0         NaN                  NaN             NaN   \n",
      "1         NaN  2012-10-16 00:00:00  09:38:08-05:00   \n",
      "2         NaN  2012-10-16 00:00:00  10:22:34-05:00   \n",
      "3         NaN  2012-10-16 00:00:00  10:14:18-05:00   \n",
      "4         NaN  2012-10-16 00:00:00  09:27:16-05:00   \n",
      "\n",
      "                                     Anootated tweet Unnamed: 4  \\\n",
      "0    1: positive, -1: negative, 0: neutral, 2: mixed      Class   \n",
      "1  Insidious!<e>Mitt Romney</e>'s Bain Helped Phi...         -1   \n",
      "2  Senior <e>Romney</e> Advisor Claims <e>Obama</...          2   \n",
      "3  .@WardBrenda @shortwave8669 @allanbourdius you...         -1   \n",
      "4  <e>Mitt Romney</e> still doesn't <a>believe</a...         -1   \n",
      "\n",
      "         Unnamed: 5  \n",
      "0  Your class label  \n",
      "1               NaN  \n",
      "2               NaN  \n",
      "3               NaN  \n",
      "4               NaN  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Adjust the path to where your file is stored on your local system\n",
    "file_path = '/Users/suryabeeraka/Downloads/583 Project Phase 2/training-Obama-Romney-tweet.xlsx'\n",
    "\n",
    "# Loading the datasets from the Excel file\n",
    "obama_tweets = pd.read_excel(file_path, sheet_name='Obama')\n",
    "romney_tweets = pd.read_excel(file_path, sheet_name='Romney')\n",
    "\n",
    "# Display the first few rows of each dataset to inspect\n",
    "print(\"Obama Tweets:\")\n",
    "print(obama_tweets.head(), '\\n')  # Print first 5 rows of Obama tweets\n",
    "\n",
    "print(\"Romney Tweets:\")\n",
    "print(romney_tweets.head())  # Print first 5 rows of Romney tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3949d01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/suryabeeraka/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/suryabeeraka/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/suryabeeraka/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/suryabeeraka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Downloading necessary NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initializing the lemmatizer and stopwords list\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93686fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama Tweets columns: ['Unnamed: 0', 'date', 'time', 'Anootated tweet', 'Unnamed: 4', 'Unnamed: 5']\n",
      "Romney Tweets columns: ['Unnamed: 0', 'date', 'time', 'Anootated tweet', 'Unnamed: 4', 'Unnamed: 5']\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from both sheets\n",
    "file_path = '/Users/suryabeeraka/Downloads/583 Project Phase 2/training-Obama-Romney-tweet.xlsx'\n",
    "obama_tweets = pd.read_excel(file_path, sheet_name='Obama')\n",
    "romney_tweets = pd.read_excel(file_path, sheet_name='Romney')\n",
    "\n",
    "# Print out the column names to verify\n",
    "print(\"Obama Tweets columns:\", obama_tweets.columns.tolist())\n",
    "print(\"Romney Tweets columns:\", romney_tweets.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce854f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Obama Tweets:\n",
      "                                               Tweet Class\n",
      "1  Kirkpatrick, who wore a baseball cap embroider...     0\n",
      "3  #<e>obama</e> debates that Cracker Ass Cracker...     1\n",
      "5  @Hollivan @hereistheanswer  Youre missing the ...     0\n",
      "7  I was raised as a Democrat  left the party yea...    -1\n",
      "8  The <e>Obama camp</e> can't afford to lower ex...     0 \n",
      "\n",
      "Cleaned Romney Tweets:\n",
      "                                               Tweet Class  \\\n",
      "1  Insidious!<e>Mitt Romney</e>'s Bain Helped Phi...    -1   \n",
      "3  .@WardBrenda @shortwave8669 @allanbourdius you...    -1   \n",
      "4  <e>Mitt Romney</e> still doesn't <a>believe</a...    -1   \n",
      "5  <e>Romney</e>'s <a>tax plan</a> deserves a 2nd...    -1   \n",
      "6  Hope <e>Romney</e> debate prepped w/ the same ...     1   \n",
      "\n",
      "                                     Processed_Tweet Encoded_Class  \n",
      "1  insidi ! mitt romney 's bain help philip morri...          None  \n",
      "3  . @ wardbrenda @ shortwave8669 @ allanbourdiu ...          None  \n",
      "4        mitt romney still n't believ black presid .          None  \n",
      "5  romney 's tax plan deserv 2nd look secret one ...          None  \n",
      "6        hope romney debat prep w/ peopl last time .          None  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "def clean_data(sheet, class_column_name, sentiment_col):\n",
    "    # Check if sentiment column exists, handle if not\n",
    "    if sentiment_col not in sheet.columns:\n",
    "        print(f\"Column {sentiment_col} not found in the DataFrame.\")\n",
    "        return None  # or handle appropriately\n",
    "\n",
    "    # Remove rows with mixed sentiments (class '2')\n",
    "    sheet = sheet[sheet[sentiment_col].isin([-1, 0, 1])]\n",
    "    \n",
    "    # Drop the 'Your class' or 'Your class label' column, if necessary\n",
    "    if class_column_name in sheet.columns:\n",
    "        sheet = sheet.drop(columns=[class_column_name])\n",
    "\n",
    "    # Assuming 'Anootated tweet' is the correct tweet column (watch for typos)\n",
    "    sheet = sheet.rename(columns={'Anootated tweet': 'Tweet'})\n",
    "\n",
    "    # Keeping only the 'Tweet' and specified sentiment columns for analysis\n",
    "    sheet = sheet[['Tweet', sentiment_col]]\n",
    "    sheet = sheet.rename(columns={sentiment_col: 'Class'})  # Standardizing the class column name for consistency\n",
    "\n",
    "    return sheet\n",
    "print(\"Cleaned Obama Tweets:\")\n",
    "print(obama_tweets_clean.head(), '\\n')\n",
    "\n",
    "print(\"Cleaned Romney Tweets:\")\n",
    "print(romney_tweets_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd312d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama Tweets cleaned data sample:\n",
      "                                               Tweet Class\n",
      "1  Kirkpatrick, who wore a baseball cap embroider...     0\n",
      "2  Question: If <e>Romney</e> and <e>Obama</e> ha...     2\n",
      "3  #<e>obama</e> debates that Cracker Ass Cracker...     1\n",
      "4  RT @davewiner Slate: Blame <e>Obama</e> for fo...     2\n",
      "5  @Hollivan @hereistheanswer  Youre missing the ...     0\n"
     ]
    }
   ],
   "source": [
    "def clean_data(sheet, tweet_column_name, class_column_name):\n",
    "    # Example of checking multiple potential columns for the class, and using the first found\n",
    "    potential_class_columns = ['Unnamed: 4', 'Unnamed: 5']\n",
    "    for col in potential_class_columns:\n",
    "        if col in sheet.columns:\n",
    "            class_column_name = col\n",
    "            break\n",
    "    \n",
    "    # Remove rows if the class column contains invalid data (example assumes valid classes are integers)\n",
    "    if class_column_name in sheet.columns:\n",
    "        sheet = sheet[pd.to_numeric(sheet[class_column_name], errors='coerce').notna()]\n",
    "    \n",
    "    # Rename the tweet column for consistency\n",
    "    sheet = sheet.rename(columns={tweet_column_name: 'Tweet'})\n",
    "    sheet = sheet[['Tweet', class_column_name]]\n",
    "    sheet = sheet.rename(columns={class_column_name: 'Class'})  # Standardizing the class column name\n",
    "    \n",
    "    return sheet\n",
    "\n",
    "# Cleaning data example\n",
    "obama_tweets_clean = clean_data(obama_tweets, 'Anootated tweet', 'Class')\n",
    "print(\"Obama Tweets cleaned data sample:\")\n",
    "print(obama_tweets_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7d7ccc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Obama Tweets:\n",
      "                                     Processed_Tweet Class\n",
      "1  kirkpatrick , wore basebal cap embroid barack ...     0\n",
      "2  question : romney obama child-punch contest , ...     2\n",
      "3  # obama debat cracker ass cracker tonight ... ...     1\n",
      "4  rt @ davewin slate : blame obama four death li...     2\n",
      "5  @ hollivan @ hereistheansw your miss point im ...     0 \n",
      "\n",
      "Processed Romney Tweets:\n",
      "                                     Processed_Tweet Class\n",
      "1  insidi ! mitt romney 's bain help philip morri...    -1\n",
      "3  . @ wardbrenda @ shortwave8669 @ allanbourdiu ...    -1\n",
      "4        mitt romney still n't believ black presid .    -1\n",
      "5  romney 's tax plan deserv 2nd look secret one ...    -1\n",
      "6        hope romney debat prep w/ peopl last time .     1\n"
     ]
    }
   ],
   "source": [
    "def preprocess_tweet(text):\n",
    "    # Convert to string in case there are non-string data types\n",
    "    text = str(text)\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    # Convert to lower case\n",
    "    words = [word.lower() for word in words]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Preprocess the tweets\n",
    "obama_tweets_clean['Processed_Tweet'] = obama_tweets_clean['Tweet'].apply(preprocess_tweet)\n",
    "romney_tweets_clean['Processed_Tweet'] = romney_tweets_clean['Tweet'].apply(preprocess_tweet)\n",
    "\n",
    "# Preview the processed tweets\n",
    "print(\"Processed Obama Tweets:\")\n",
    "print(obama_tweets_clean[['Processed_Tweet', 'Class']].head(), '\\n')\n",
    "\n",
    "print(\"Processed Romney Tweets:\")\n",
    "print(romney_tweets_clean[['Processed_Tweet', 'Class']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "218fa26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming sentiment labels are in a different format and need to be encoded\n",
    "def encode_sentiment(label):\n",
    "    if label == \"positive\":\n",
    "        return 1\n",
    "    elif label == \"negative\":\n",
    "        return -1\n",
    "    elif label == \"neutral\":\n",
    "        return 0\n",
    "    # Add additional conditions if there are more labels\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Apply the encoding function to your sentiment column\n",
    "obama_tweets_clean['Encoded_Class'] = obama_tweets_clean['Class'].apply(encode_sentiment)\n",
    "romney_tweets_clean['Encoded_Class'] = romney_tweets_clean['Class'].apply(encode_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "783b4bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.54      0.59      0.57       385\n",
      "           0       0.47      0.45      0.46       401\n",
      "           1       0.59      0.56      0.58       321\n",
      "           2       0.72      0.70      0.71       327\n",
      "\n",
      "    accuracy                           0.57      1434\n",
      "   macro avg       0.58      0.58      0.58      1434\n",
      "weighted avg       0.57      0.57      0.57      1434\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suryabeeraka/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Use TF-IDF to vectorize the tweets\n",
    "tfidf = TfidfVectorizer(max_features=1000)  # Limiting to 1000 features for simplicity\n",
    "X = tfidf.fit_transform(obama_tweets_clean['Processed_Tweet']).toarray()\n",
    "y = obama_tweets_clean['Class'].astype(int)  # Ensure the class labels are integers\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "822186ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "90/90 [==============================] - 10s 101ms/step - loss: 0.9657 - accuracy: 0.3254 - val_loss: 0.8967 - val_accuracy: 0.3738\n",
      "Epoch 2/5\n",
      "90/90 [==============================] - 9s 105ms/step - loss: 0.8043 - accuracy: 0.4314 - val_loss: 0.8676 - val_accuracy: 0.4177\n",
      "Epoch 3/5\n",
      "90/90 [==============================] - 9s 104ms/step - loss: 0.6638 - accuracy: 0.5144 - val_loss: 0.9091 - val_accuracy: 0.4031\n",
      "Epoch 4/5\n",
      "90/90 [==============================] - 9s 104ms/step - loss: 0.5483 - accuracy: 0.5744 - val_loss: 0.9472 - val_accuracy: 0.4317\n",
      "Epoch 5/5\n",
      "90/90 [==============================] - 10s 109ms/step - loss: 0.4458 - accuracy: 0.6115 - val_loss: 1.1133 - val_accuracy: 0.4212\n",
      "45/45 [==============================] - 1s 17ms/step - loss: 1.1133 - accuracy: 0.4212\n",
      "Accuracy: 42.12%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sample DataFrame loading step (assuming DataFrame is ready and named as obama_tweets_clean)\n",
    "# obama_tweets_clean = pd.read_excel('path_to_your_file.xlsx')\n",
    "\n",
    "# Step 1: Relabel the classes to start from 0\n",
    "label_mapping = {-1: 0, 0: 1, 1: 2}\n",
    "obama_tweets_clean['Class'] = obama_tweets_clean['Class'].map(label_mapping)\n",
    "\n",
    "# Ensure all text data is string and not missing\n",
    "obama_tweets_clean['Tweet'] = obama_tweets_clean['Tweet'].fillna('missing').astype(str)\n",
    "tweets = obama_tweets_clean['Tweet'].values\n",
    "labels = obama_tweets_clean['Class'].values\n",
    "\n",
    "# Step 2: Tokenize and pad text data\n",
    "tokenizer = Tokenizer(num_words=5000)  # Keeps only the 5000 most frequent words\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "sequences = tokenizer.texts_to_sequences(tweets)\n",
    "X = pad_sequences(sequences, maxlen=100)  # Ensures all sequences have the same length\n",
    "y = np.array(labels)\n",
    "\n",
    "# Step 3: Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=100, input_length=100))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))  # Output layer for 3 classes\n",
    "\n",
    "# Step 5: Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Step 6: Train the model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))\n",
    "\n",
    "# Step 7: Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4b673b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.9525 - accuracy: 0.3323\n",
      "Epoch 1: val_loss improved from inf to 0.87781, saving model to best_model.h5\n",
      "90/90 [==============================] - 36s 378ms/step - loss: 0.9525 - accuracy: 0.3323 - val_loss: 0.8778 - val_accuracy: 0.3905\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suryabeeraka/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - ETA: 0s - loss: 0.7742 - accuracy: 0.4638\n",
      "Epoch 2: val_loss improved from 0.87781 to 0.86917, saving model to best_model.h5\n",
      "90/90 [==============================] - 48s 532ms/step - loss: 0.7742 - accuracy: 0.4638 - val_loss: 0.8692 - val_accuracy: 0.4107\n",
      "Epoch 3/10\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.6195 - accuracy: 0.5416\n",
      "Epoch 3: val_loss did not improve from 0.86917\n",
      "90/90 [==============================] - 47s 527ms/step - loss: 0.6195 - accuracy: 0.5416 - val_loss: 0.8878 - val_accuracy: 0.4184\n",
      "Epoch 4/10\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.5010 - accuracy: 0.5965\n",
      "Epoch 4: val_loss did not improve from 0.86917\n",
      "90/90 [==============================] - 47s 518ms/step - loss: 0.5010 - accuracy: 0.5965 - val_loss: 0.9903 - val_accuracy: 0.4052\n",
      "Epoch 5/10\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.4055 - accuracy: 0.6389\n",
      "Epoch 5: val_loss did not improve from 0.86917\n",
      "90/90 [==============================] - 49s 547ms/step - loss: 0.4055 - accuracy: 0.6389 - val_loss: 1.1711 - val_accuracy: 0.3975\n",
      "Epoch 5: early stopping\n",
      "45/45 [==============================] - 4s 84ms/step - loss: 0.8692 - accuracy: 0.4107\n",
      "Accuracy: 41.07%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "# Updated LSTM model with bidirectional layer and increased complexity\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=5000, output_dim=100, input_length=100))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Consider using early stopping and model checkpointing\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
    "checkpoint = ModelCheckpoint( 'best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stop, checkpoint])\n",
    "\n",
    "# Load the best model and evaluate it\n",
    "model.load_weights('best_model.h5')\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "df73a4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Tweets for Obama:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'all_tweets_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Display the first few entries for each candidate\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample Tweets for Obama:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28mprint\u001b[39m(all_tweets_data[all_tweets_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCandidate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObama\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhead())\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSample Tweets for Romney:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(all_tweets_data[all_tweets_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCandidate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRomney\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_tweets_data' is not defined"
     ]
    }
   ],
   "source": [
    "def prepare_tweet_data(file_path, sheet_name, tweet_col_letter, class_col_letter, excluded_class, identifier):\n",
    "    # Convert Excel column letter to index (zero-based)\n",
    "    def excel_col_to_index(col_letter):\n",
    "        return pd.io.excel._excel._letter_to_num(col_letter.lower()) - 1\n",
    "    \n",
    "    # Define column indices for pandas\n",
    "    cols_to_use = f\"{tweet_col_letter}:{class_col_letter}\"\n",
    "    \n",
    "    # Load specific columns from the Excel sheet\n",
    "    tweets_df = pd.read_excel(file_path, sheet_name=sheet_name, usecols=cols_to_use)\n",
    "    \n",
    "    # Set meaningful column names\n",
    "    tweets_df.columns = [\"TweetContent\", \"Sentiment\"]\n",
    "    \n",
    "    # Exclude tweets with the specified class\n",
    "    tweets_df = tweets_df[tweets_df['Sentiment'] != excluded_class]\n",
    "    \n",
    "    # Replace missing tweet content with a placeholder\n",
    "    tweets_df['TweetContent'] = tweets_df['TweetContent'].fillna(\"UNKNOWN\")\n",
    "    \n",
    "    # Add a column for the candidate's name\n",
    "    tweets_df['Candidate'] = identifier\n",
    "    \n",
    "    return tweets_df[['Candidate', 'TweetContent', 'Sentiment']]\n",
    "data_source_path = \"/Users/suryabeeraka/Downloads/CS583_Project/training-Obama-Romney-tweets.xlsx\"\n",
    "obama_data_sheet  = \"Obama\"   \n",
    "romney_data_sheet = \"Romney\"\n",
    "tweet_col_id, sentiment_col_id = \"D\", \"E\"  # Excel column letters for tweets and sentiment labels\n",
    "\n",
    "# Load and process tweet data for analysis\n",
    "data_obama = prepare_tweet_data(data_source_path, obama_data_sheet, tweet_col_id, sentiment_col_id, 2, 'Barack Obama')\n",
    "data_romney = prepare_tweet_data(data_source_path, romney_data_sheet, tweet_col_id, sentiment_col_id, 2, 'Mitt Romney')\n",
    "\n",
    "# Aggregate processed data from both political figures into a single data structure\n",
    "combined_sentiment_data = pd.concat([data_obama, data_romney], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "# Display the first few entries for each candidate\n",
    "print(\"Sample Tweets for Obama:\")\n",
    "print(all_tweets_data[all_tweets_data['Candidate'] == 'Obama'].head())\n",
    "\n",
    "print(\"\\nSample Tweets for Romney:\")\n",
    "print(all_tweets_data[all_tweets_data['Candidate'] == 'Romney'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "891e63cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Data for Obama:\n",
      "                                        TweetContent Sentiment\n",
      "1  Kirkpatrick, who wore a baseball cap embroider...         0\n",
      "3  #<e>obama</e> debates that Cracker Ass Cracker...         1\n",
      "5  @Hollivan @hereistheanswer  Youre missing the ...         0\n",
      "7  I was raised as a Democrat  left the party yea...        -1\n",
      "8  The <e>Obama camp</e> can't afford to lower ex...         0\n",
      "Cleaned Data for Romney:\n",
      "                                        TweetContent Sentiment\n",
      "1  Insidious!<e>Mitt Romney</e>'s Bain Helped Phi...        -1\n",
      "3  .@WardBrenda @shortwave8669 @allanbourdius you...        -1\n",
      "4  <e>Mitt Romney</e> still doesn't <a>believe</a...        -1\n",
      "5  <e>Romney</e>'s <a>tax plan</a> deserves a 2nd...        -1\n",
      "6  Hope <e>Romney</e> debate prepped w/ the same ...         1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_and_clean_tweet_data(file_path, sheet_name, tweet_col_letter, class_col_letter, excluded_classes):\n",
    "    # Helper function to convert Excel column letter to index (zero-based)\n",
    "    def excel_col_to_index(col_letter):\n",
    "        return pd.io.excel._excel._letter_to_num(col_letter.lower()) - 1\n",
    "\n",
    "    # Load specific columns from the Excel sheet\n",
    "    cols_to_use = f\"{tweet_col_letter}:{class_col_letter}\"\n",
    "    tweets_df = pd.read_excel(file_path, sheet_name=sheet_name, usecols=cols_to_use)\n",
    "    \n",
    "    # Set meaningful column names\n",
    "    tweets_df.columns = [\"TweetContent\", \"Sentiment\"]\n",
    "    \n",
    "    # Filter out tweets with the specified class and replace missing tweet content\n",
    "    tweets_df = tweets_df[tweets_df['Sentiment'].isin(excluded_classes)].copy()\n",
    "    tweets_df.loc[:, 'TweetContent'] = tweets_df['TweetContent'].fillna(\"UNKNOWN\")\n",
    "    \n",
    "    return tweets_df\n",
    "\n",
    "# Example usage:\n",
    "data_source_path = \"/Users/suryabeeraka/Downloads/CS583_Project/training-Obama-Romney-tweets.xlsx\"\n",
    "tweet_col_id, sentiment_col_id = \"D\", \"E\"\n",
    "\n",
    "# Define the classes to include (exclude class 2)\n",
    "included_classes = [0, 1, -1]\n",
    "\n",
    "# Clean data for Obama\n",
    "obama_data_sheet = \"Obama\"\n",
    "obama_cleaned_tweets = load_and_clean_tweet_data(data_source_path, obama_data_sheet, tweet_col_id, sentiment_col_id, included_classes)\n",
    "print(\"Cleaned Data for Obama:\")\n",
    "print(obama_cleaned_tweets.head())\n",
    "\n",
    "# Clean data for Romney\n",
    "romney_data_sheet = \"Romney\"\n",
    "romney_cleaned_tweets = load_and_clean_tweet_data(data_source_path, romney_data_sheet, tweet_col_id, sentiment_col_id, included_classes)\n",
    "print(\"Cleaned Data for Romney:\")\n",
    "print(romney_cleaned_tweets.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da67b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Part-of-speech mapping for lemmatization\n",
    "pos_mapping = {\n",
    "    \"N\": 'n',  # Noun\n",
    "    \"V\": 'v',  # Verb\n",
    "    \"J\": 'a',  # Adjective\n",
    "    \"R\": 'r'   # Adverb\n",
    "}\n",
    "\n",
    "def preprocess_and_process_tweet_data(tweets_df, identifier):\n",
    "    # Ensure that modifications do not affect the original DataFrame outside this function\n",
    "    tweets_df = tweets_df.copy()\n",
    "\n",
    "    # Preprocessing tweets\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+|\\d+:\\d+[ap]m|…\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def clean_and_lemmatize(text):\n",
    "        if pd.isna(text):\n",
    "            text = ''  # Replace NaN with empty string\n",
    "        else:\n",
    "            text = text.lower().replace(\"'s\", \"\").replace(\"'\", \"\").replace(\"-\", \" \")\n",
    "            text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        lemmatized = [lemmatizer.lemmatize(token, pos_mapping.get(pos[0], 'n')) for token, pos in pos_tags if token not in stop_words]\n",
    "        return ' '.join(lemmatized)\n",
    "\n",
    "    # Apply the cleaning and lemmatization function to each tweet\n",
    "    tweets_df['TweetContent'] = tweets_df['TweetContent'].apply(clean_and_lemmatize)\n",
    "\n",
    "    # Add a column for the candidate's name\n",
    "    tweets_df.loc[:, 'Candidate'] = identifier\n",
    "\n",
    "    return tweets_df[['Candidate', 'TweetContent', 'Sentiment']]\n",
    "\n",
    "# Example usage\n",
    "data_source_path = \"/Users/suryabeeraka/Downloads/CS583_Project/training-Obama-Romney-tweets.xlsx\"\n",
    "# Load and filter the data to exclude sentiment class 2\n",
    "obama_tweets = pd.read_excel(data_source_path, sheet_name=\"Obama\", usecols=\"D:E\")\n",
    "obama_tweets.columns = ['TweetContent', 'Sentiment']\n",
    "obama_tweets = obama_tweets[obama_tweets['Sentiment'] != 2]  # Exclude class 2\n",
    "\n",
    "romney_tweets = pd.read_excel(data_source_path, sheet_name=\"Romney\", usecols=\"D:E\")\n",
    "romney_tweets.columns = ['TweetContent', 'Sentiment']\n",
    "romney_tweets = romney_tweets[romney_tweets['Sentiment'] != 2]  # Exclude class 2\n",
    "\n",
    "# Process and display formatted data for Obama\n",
    "obama_processed_tweets = preprocess_and_process_tweet_data(obama_tweets, 'Barack Obama')\n",
    "obama_processed_tweets['TweetContent'] = obama_processed_tweets['TweetContent'].str.replace(' ', ', ')\n",
    "print(\"Processed Data for Obama:\")\n",
    "print(obama_processed_tweets.head())\n",
    "\n",
    "# Process and display formatted data for Romney\n",
    "romney_processed_tweets = preprocess_and_process_tweet_data(romney_tweets, 'Mitt Romney')\n",
    "romney_processed_tweets['TweetContent'] = romney_processed_tweets['TweetContent'].str.replace(' ', ', ')\n",
    "print(\"\\nProcessed Data for Romney:\")\n",
    "print(romney_processed_tweets.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879ea552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f5e553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(data_obama['TweetContent'])\n",
    "X = tokenizer.texts_to_sequences(data_obama['TweetContent'])\n",
    "X = pad_sequences(X, maxlen=50)\n",
    "\n",
    "# Prepare labels\n",
    "y = pd.get_dummies(data_obama['Sentiment']).values\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, 128, input_length=X.shape[1]))\n",
    "model.add(LSTM(50, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))  # Assuming 3 classes -1, 0, 1\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "_, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"LSTM Accuracy Score:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d89f320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3e4aee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
